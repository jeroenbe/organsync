{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "import os, math\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "dir_path = Path(os.getcwd()).absolute()\n",
    "module_path = str(dir_path.parent.parent.parent)\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxpy as cp\n",
    "import networkx as nx\n",
    "import torch, wandb\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "pl.utilities.seed.seed_everything(seed=0)\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import rc, ticker\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter, FormatStrFormatter\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "rc('text', usetex=True)\n",
    "rc('font', family='serif')\n",
    "\n",
    "# OWN MODULES\n",
    "from organsync.models.organsync_network import OrganSync_Network\n",
    "from experiments.data.data_module import UNOSDataModule, UKRegDataModule, UNOS2UKRegDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PARAMS\n",
    "data = 'UKreg'\n",
    "batch_size = 256\n",
    "synth=False\n",
    "n_models=1\n",
    "# MODEL PARAMS\n",
    "sweep_name='qhybu27g'\n",
    "# sweep_name UKReg (ss):   u6yduuey\n",
    "# sweep_name U2U (ss):     pfkuu2l6\n",
    "# sweep_name UKreg (fact): kafit9ga\n",
    "# sweep_name U2U (fact):   qhybu27g\n",
    "\n",
    "\n",
    "# LOAD DATA\n",
    "if data == 'UNOS':\n",
    "    project = 'organsync-net'\n",
    "    data_dir = '../data/processed'\n",
    "    dm = UNOSDataModule(data_dir, batch_size=batch_size, is_synth=synth)\n",
    "elif data == 'U2U':\n",
    "    project = 'organsync-net-u2u'\n",
    "    data_dir = '../data/processed_UNOS2UKReg_no_split'\n",
    "    dm = UNOS2UKRegDataModule(data_dir, batch_size=batch_size, is_synth=synth)\n",
    "    dm.prepare_data()\n",
    "else:\n",
    "    project = 'organsync-net-ukreg'\n",
    "    data_dir = '../data/processed_UKReg/clinical_ukeld_2_ukeld'\n",
    "    dm = UKRegDataModule(data_dir, batch_size=batch_size, is_synth=synth)\n",
    "    dm.prepare_data()\n",
    "\n",
    "dm.setup(stage='test')\n",
    "dm.setup(stage='fit')    \n",
    "\n",
    "# LOAD MODELS\n",
    "api = wandb.Api()\n",
    "sweep = api.sweep(path=f'jeroenbe/{project}/{sweep_name}')\n",
    "                \n",
    "summary_list = []\n",
    "for run in sweep.runs:\n",
    "    d = run.summary._json_dict\n",
    "    d['id'] = run.id\n",
    "    summary_list.append(d)\n",
    "                        \n",
    "\n",
    "summary_df = pd.DataFrame.from_records(summary_list)\n",
    "model_ids = summary_df.nsmallest(n=n_models, columns=['test_loss (synth) - RMSE']).id.to_numpy().astype(str)\n",
    "\n",
    "models = dict()\n",
    "for model_id in model_ids:\n",
    "    try:\n",
    "        params = wandb.restore(f'organsync_net.ckpt.ckpt', run_path=f'jeroenbe/{project}/{model_id}', replace=True)\n",
    "    except:\n",
    "        params = wandb.restore(f'organsync_net.ckpt-v0.ckpt', run_path=f'jeroenbe/{project}/{model_id}', replace=True)\n",
    "    \n",
    "    models[model_id] = OrganSync_Network.load_from_checkpoint(params.name).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD PATIENT CLUSTERS\n",
    "\n",
    "resolution_k = 10            # NOTE: also used for bipartite size\n",
    "n_per_cluster = 30           # NOTE: limit computation\n",
    "test_length = len(dm.test_dataloader().dataset)\n",
    "train_length = len(dm.train_dataloader().dataset)\n",
    "\n",
    "X, O, Y, _ = dm.train_dataloader().dataset.dataset.tensors\n",
    "X_test, O_test, Y_test, _ = dm.test_dataloader().dataset.tensors\n",
    "XO = torch.cat((X, O), dim=1)\n",
    "\n",
    "cluster_x = KMeans(n_clusters=resolution_k)\n",
    "cluster_xo= KMeans(n_clusters=resolution_k)\n",
    "\n",
    "cluster_x.fit(X)\n",
    "cluster_xo.fit(XO)\n",
    "\n",
    "print('Size of c(X):', Counter(cluster_x.labels_))\n",
    "print('Size of c(XO):', Counter(cluster_xo.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT PATIENTS FROM CLUSTERS\n",
    "# this is done on the test set\n",
    "\n",
    "cluster_labels = np.arange(0, resolution_k, 1)\n",
    "\n",
    "patients_x = np.empty((0, n_per_cluster), dtype=int)   # INFO: each index corresponds with a label\n",
    "patients_xo = np.empty((0, n_per_cluster), dtype=int)  #   for each label, we sample 20 patients.\n",
    "                                                       #   This is to limit computation, more can \n",
    "                                                       #   be sampled, though 200 patients is likely \n",
    "                                                       #   enough.\n",
    "            \n",
    "X_test, O_test,_,_ = dm.test_dataloader().dataset.tensors\n",
    "test_cluster_labels_x = cluster_x.predict(X_test)  \n",
    "test_cluster_labels_xo= cluster_xo.predict(torch.cat((X_test, O_test), dim=1))\n",
    "\n",
    "for label in cluster_labels:\n",
    "    patients_of_label_x = np.where(test_cluster_labels_x == label)[0]\n",
    "    patients_in_label_x = patients_of_label_x[np.random.randint(0, len(patients_of_label_x), (n_per_cluster,))]\n",
    "    \n",
    "    patients_of_label_xo = np.where(test_cluster_labels_xo == label)[0]\n",
    "    patients_in_label_xo = patients_of_label_xo[np.random.randint(0, len(patients_of_label_xo), (n_per_cluster,))]\n",
    "    \n",
    "    patients_xo = np.append(patients_xo, patients_in_label_xo.reshape(1, -1), axis=0)\n",
    "    patients_x = np.append(patients_x, patients_in_label_x.reshape(1, -1), axis=0)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR EACH MODEL BUILD U\n",
    "# this is done on the training set\n",
    "\n",
    "n = 300      # limit compute\n",
    "U = dict()   # representation\n",
    "\n",
    "for k in models.keys():\n",
    "    indxs = torch.randint(0, len(X), (n,))\n",
    "    catted = torch.cat((X[indxs], O[indxs]), dim=1).double()\n",
    "    U_labels_x = cluster_x.predict(X[indxs])\n",
    "    U_labels_xo = cluster_xo.predict(catted)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        representation = models[k].representation(catted)\n",
    "    \n",
    "    U[k] = representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER CLUSTER, PER MODEL BUILD u\n",
    "\n",
    "# new pair representation per model -> (resolution_k, n_per_cluster, dim(u))\n",
    "u_x = {k: np.empty((0, n_per_cluster, models[k].output_dim)) for k in models.keys()}\n",
    "u_xo = {k: np.empty((0, n_per_cluster, models[k].output_dim)) for k in models.keys()}\n",
    "\n",
    "\n",
    "# REPRESENT X\n",
    "for ps in patients_x:\n",
    "\n",
    "    x, o, _, _ = dm.train_dataloader().dataset.dataset[ps]\n",
    "    pair = torch.cat((x, o), dim=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for k in models.keys():\n",
    "            representation = models[k].representation(pair).view(1, n_per_cluster, -1)\n",
    "            u_x[k] = np.append(u_x[k], representation, axis=0)\n",
    "\n",
    "# REPRESENT XO\n",
    "for ps in patients_xo:\n",
    "    x, o, _, _ = dm.train_dataloader().dataset.dataset[ps]\n",
    "    pair = torch.cat((x, o), dim=1)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for k in models.keys():\n",
    "            representation = models[k].representation(pair)\n",
    "            u_xo[k] = np.append(u_xo[k], representation.view(1, n_per_cluster, -1), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PER CLUSTER, PER MODEL COMPUTE a\n",
    "# NOTE: this cell comprises the bulk of \n",
    "#   the computation; might run long.\n",
    "\n",
    "\n",
    "lambda_ = .1\n",
    "\n",
    "\n",
    "# a_s per model -> (resolution_k, n_per_cluster, len(U)) => per (cluster, patient, a_s)\n",
    "A_s_x = {k: np.empty((0, n_per_cluster, n)) for k in models.keys()} \n",
    "A_s_xo = {k: np.empty((0, n_per_cluster, n)) for k in models.keys()} \n",
    "\n",
    "\n",
    "def convex_opt(u, U, lambd):\n",
    "    a = cp.Variable(U.shape[0])\n",
    "\n",
    "    objective = cp.Minimize(cp.norm2(a@U - u)**2 + lambd * cp.norm1(a))\n",
    "    constraints = [0 <= a, a <= 1, cp.sum(a) == 1]\n",
    "    prob = cp.Problem(objective, constraints)\n",
    "\n",
    "    _ = prob.solve(warm_start=True, solver=cp.SCS)\n",
    "\n",
    "    return a.value\n",
    "\n",
    "\n",
    "print('-- STARTING --')\n",
    "print('---- CVX OPT for X')\n",
    "for k in u_x.keys():\n",
    "    for u_s_in_cluster in u_x[k]:\n",
    "        a = Parallel(n_jobs=joblib.cpu_count())(delayed(convex_opt)(u_, U[k], lambda_) for u_ in u_s_in_cluster)\n",
    "        A_s_x[k] = np.append(A_s_x[k], np.array(a)[:].reshape(1, n_per_cluster, -1), axis=0)\n",
    "    print(f'------ finished model {k}')\n",
    "        \n",
    "        \n",
    "\n",
    "print('---- CVX OPT for XO')\n",
    "for k in u_xo.keys():\n",
    "    for u_s_in_cluster in u_xo[k]:\n",
    "        a = Parallel(n_jobs=joblib.cpu_count())(delayed(convex_opt)(u_, U[k], lambda_) for u_ in u_s_in_cluster)\n",
    "        A_s_xo[k] = np.append(A_s_xo[k], np.array(a)[:].reshape(1, n_per_cluster, -1), axis=0)\n",
    "    print(f'------ finished model {k}')\n",
    "print('-- FINISHED --')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BUILD MATRIX FROM a\n",
    "#    INFO: every cluster connects to every other cluster.\n",
    "#      On each row, there is the amount of the column\n",
    "#      the cluster has on other clusters.\n",
    "threshold = 1e-1 \n",
    "\n",
    "\n",
    "M_x = np.empty((resolution_k, resolution_k))\n",
    "M_xo= np.empty((resolution_k, resolution_k))\n",
    "\n",
    "\n",
    "# X\n",
    "combined_x = np.array(list(A_s_x.values()))\n",
    "\n",
    "# filtered = [models, clusters, patients, a]\n",
    "filtered_x = np.where(combined_x > threshold, combined_x, np.zeros(combined_x.shape))\n",
    "\n",
    "for i, r in enumerate(M_x):\n",
    "    one_model_sample = np.repeat(U_labels_x[np.newaxis, :], n_per_cluster, axis=0)\n",
    "    more_model_sample= np.repeat(one_model_sample[np.newaxis, :, :], n_models, axis=0)\n",
    "\n",
    "    sample = more_model_sample[filtered_x[:,i,:,:].astype(bool)]\n",
    "\n",
    "    unique, counts = np.unique(sample, return_counts=True)\n",
    "    label_distribution = dict(zip(unique, counts))\n",
    "    \n",
    "    M_x[i, list(label_distribution.keys())] = list(label_distribution.values())\n",
    "\n",
    "M_x = normalize(M_x, axis=1, norm='l1')\n",
    "        \n",
    "\n",
    "# XO\n",
    "combined_xo = np.array(list(A_s_xo.values()))\n",
    "\n",
    "# filtered = [models, clusters, patients, a]\n",
    "filtered_xo = np.where(combined_xo > threshold, combined_xo, np.zeros(combined_xo.shape))\n",
    "\n",
    "for i, r in enumerate(M_xo):\n",
    "    one_model_sample = np.repeat(U_labels_xo[np.newaxis, :], n_per_cluster, axis=0)\n",
    "    more_model_sample= np.repeat(one_model_sample[np.newaxis, :, :], n_models, axis=0)\n",
    "\n",
    "    sample = more_model_sample[filtered_xo[:,i,:,:].astype(bool)]\n",
    "\n",
    "    unique, counts = np.unique(sample, return_counts=True)\n",
    "    label_distribution = dict(zip(unique, counts))\n",
    "    \n",
    "    M_xo[i, list(label_distribution.keys())] = list(label_distribution.values())\n",
    "\n",
    "M_xo = normalize(M_xo, axis=1, norm='l1')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT MATRIX (XO)\n",
    "def plot_matrix(m, require_colorbar, title):\n",
    "    rc('axes', linewidth= 4.5) \n",
    "\n",
    "    require_colorbar = True\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "\n",
    "    ax.set_yticks(np.arange(0,10,1))\n",
    "    ax.set_yticklabels(np.arange(0, 10, 1), fontsize=25)\n",
    "    ax.set_xticks(np.arange(0,10,1))\n",
    "    ax.set_xticklabels(np.arange(0, 10, 1), fontsize=25)\n",
    "\n",
    "    ax.set_ylabel('composed of',  fontsize=20)\n",
    "    ax.set_xlabel('contributes to',  fontsize=20)\n",
    "\n",
    "    ax.tick_params(length=10, width=2)    \n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    im = ax.imshow(m)\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "\n",
    "\n",
    "    ax.set_title(title, fontsize=25)\n",
    "\n",
    "    if require_colorbar:\n",
    "        cax = divider.append_axes(\"right\", size='10%', pad=.2)\n",
    "        cax.tick_params(length=5, width=1)\n",
    "        \n",
    "        print(cax.yaxis.get_ticklabels())\n",
    "\n",
    "        fig.colorbar(im, cax=cax, ticks=np.arange(0, .2, .04))\n",
    "        cax.set_yticklabels(np.arange(0, .2, .04), fontsize=15)\n",
    "        \n",
    "    return fig\n",
    "\n",
    "f = plot_matrix(M_xo, True, 'UKReg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE FIGURE\n",
    "# SAVE RESULTS\n",
    "fig_detail='UKReg'\n",
    "\n",
    "\n",
    "f.savefig(f'{fig_detail}_composition.pdf', bbox_inches = \"tight\")\n",
    "np.save(f'{fig_detail}_M_xo', M_xo)\n",
    "np.save(f'{fig_detail}_M_x', M_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='UKReg (semi-synth.)'\n",
    "\n",
    "m_xo_unos = np.load(f'./{name}_M_xo.npy')\n",
    "f = plot_matrix(m_xo_unos, True, name)\n",
    "f.savefig(f'{name}_composition.pdf', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.colorbar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "organsync",
   "language": "python",
   "name": "organsync"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
